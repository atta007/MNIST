# -*- coding: utf-8 -*-
"""MNIST_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1igpYrrmRa41hg19u95hvix4tc7k2AauE
"""

import torch 
import torch.nn as nn
import torchvision 

import matplotlib.pyplot as plt

#torchvision.datasets is used to download and import the data-set 
#while torch.utils.data.DataLoader returns an iterator over the data-set.
#tranform use: The entire array is converted to torch tensor and then divided by 255.

from torchvision import datasets, transforms
train_dataset = torchvision.datasets.MNIST(root='./data',train=True, transform=transforms.ToTensor(), download=True)

test_dataset = torchvision.datasets.MNIST(root='./data',train=False, transform=transforms.ToTensor(),download = True)

#The batch size is the number of images we get in one iteration from the data loader and pass through our network, often called a batch. And shuffle=True 

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)  

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)

dataiter = iter(train_loader)
images, labels = dataiter.next()

plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');

class NeuralNet(nn.Module):
	"""A Neural Network with a hidden layer"""
	def __init__(self, input_layer,hidden_layer,output_layer):
          super(NeuralNet, self).__init__()
          self.layer1 = nn.Linear(input_layer, hidden_layer)
          self.layer2 = nn.Linear(hidden_layer, output_layer)
          self.relu = nn.ReLU()

	def forward(self, x):
          output = self.layer1(x)
          output = self.relu(output)
          output = self.layer2(output)
          return output

#define out loss function and optimizer. 
 #Cross entropy loss and adam is used.
 #also need to define our network hyper-parameters.
  
  
input_layer = 784
hidden_layer = 500
output_layer = 10
num_epochs = 5

learning_rate = 0.001

model = NeuralNet(input_layer,hidden_layer, output_layer)

lossFunction = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

total_step = len(train_loader)
for epoch in range(num_epochs):
    for i, (images,labels) in enumerate(train_loader):
        images = images.reshape(-1,28*28)
        
        out = model(images)
        loss = lossFunction(out,labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        

        if (i+1) % 100 == 0:
            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' .format(epoch+1, num_epochs, i+1, total_step, loss.item()))
            
#We have now successfully trained our network. Next we need to calculate percentage accuracy of our network on the test data.

with torch.no_grad():
  correct = 0
  total = 0
  for images,labels in test_loader:
    images = images.reshape(-1 , 28*28)
    out = model(images)
    _,predicted = torch.max(out.data,1)
    total += labels.size(0)
    correct += (predicted==labels).sum().item()
    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))

!wget -c https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/intro-to-pytorch/helper.py

# %matplotlib inline
import helper

images, labels = next(iter(train_loader))

img = images[0].view(1, 784)
# Turn off gradients to speed up this part
with torch.no_grad():
    logps = model(img)

# Output of the network are log-probabilities, need to take exponential for probabilities
ps = torch.exp(logps)
helper.view_classify(img.view(1, 28, 28), ps)